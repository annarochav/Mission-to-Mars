{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "759eabed",
   "metadata": {},
   "source": [
    "# Scrape Mars Data: The News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "650da6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Splinter and BeautifulSoup\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1d0b5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 102.0.5005\n",
      "[WDM] - Get LATEST chromedriver version for 102.0.5005 google-chrome\n",
      "[WDM] - Driver [C:\\Users\\annar\\.wdm\\drivers\\chromedriver\\win32\\102.0.5005.61\\chromedriver.exe] found in cache\n"
     ]
    }
   ],
   "source": [
    "#set your executable path, then set up the URL NASA Mars News for scraping.\n",
    "\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeff149c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mars_news(browser):\n",
    "\n",
    "    # Scrape Mars News\n",
    "    # Visit the mars nasa news site\n",
    "    url = 'https://redplanetscience.com/'\n",
    "    browser.visit(url)\n",
    "\n",
    "    # Optional delay for loading the page\n",
    "    browser.is_element_present_by_css('div.list_text', wait_time=1)\n",
    "\n",
    "    # Convert the browser html to a soup object and then quit the browser\n",
    "    html = browser.html\n",
    "    news_soup = soup(html, 'html.parser')\n",
    "\n",
    "    # Add try/except for error handling\n",
    "    try:\n",
    "        slide_elem = news_soup.select_one('div.list_text')\n",
    "        # Use the parent element to find the first 'a' tag and save it as 'news_title'\n",
    "        news_title = slide_elem.find('div', class_='content_title').get_text()\n",
    "        # Use the parent element to find the paragraph text\n",
    "        news_p = slide_elem.find('div', class_='article_teaser_body').get_text()\n",
    "\n",
    "    except AttributeError:\n",
    "        return None, None\n",
    "\n",
    "    return news_title, news_p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc825560",
   "metadata": {},
   "source": [
    "# Scrape Mars Data: Featured Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bf6dcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featured_image(browser):\n",
    "    # Visit URL\n",
    "    url = 'https://spaceimages-mars.com'\n",
    "    browser.visit(url)\n",
    "\n",
    "    # Find and click the full image button\n",
    "    full_image_elem = browser.find_by_tag('button')[1]\n",
    "    full_image_elem.click()\n",
    "\n",
    "    # Parse the resulting html with soup\n",
    "    html = browser.html\n",
    "    img_soup = soup(html, 'html.parser')\n",
    "\n",
    "    # Add try/except for error handling\n",
    "    try:\n",
    "        # Find the relative image url\n",
    "        img_url_rel = img_soup.find('img', class_='fancybox-image').get('src')\n",
    "\n",
    "    except AttributeError:\n",
    "        return None\n",
    "\n",
    "    # Use the base url to create an absolute url\n",
    "    img_url = f'https://spaceimages-mars.com/{img_url_rel}'\n",
    "\n",
    "    return img_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c8554d",
   "metadata": {},
   "source": [
    "# Scrape Mars Data: Mars Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23fa2c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mars_facts():\n",
    "    # Add try/except for error handling\n",
    "    try:\n",
    "        # Use 'read_html' to scrape the facts table into a dataframe\n",
    "        df = pd.read_html('https://galaxyfacts-mars.com')[0]\n",
    "\n",
    "    except BaseException:\n",
    "        return None\n",
    "\n",
    "    # Assign columns and set index of dataframe\n",
    "    df.columns=['Description', 'Mars', 'Earth']\n",
    "    df.set_index('Description', inplace=True)\n",
    "\n",
    "    # Convert dataframe into HTML format, add bootstrap\n",
    "    return df.to_html()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe82267f",
   "metadata": {},
   "source": [
    "# Export to Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abf794fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can't automate the scraping using the Jupyter Notebook. \n",
    "# To fully automate it, it will need to be converted into a .py file.\n",
    "# The next step in making this an automated process is to download the current code into a Python file.\n",
    "# it won't transition over perfectly, we'll need to clean it up a bit\n",
    "#you need to delete the unnecesary comments and your done!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1741a2cd",
   "metadata": {},
   "source": [
    "# Store the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "258df57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to store the data in a spot where they can be easily accessed and retrieved as needed. \n",
    "# SQL isn't a good option because it works with tabular data.\n",
    "# Mongo, a NoSQL database, is designed for exactly this task.\n",
    "# Mongo is a non-relational database that stores data in Binary JavaScript Object Notation (JSON), or BSON format.\n",
    "# We'll access data stored in Mongo the same way we access data stored in JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfdafc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A Mongo database contains collections. \n",
    "#These collections contain documents, and each document contains fields, and fields are where the data is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cea43d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68363b75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee024dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a961193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e05a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57acf1f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb61eeb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d17d439",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
